# -*- coding: utf-8 -*-
"""Data_Cleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SmJB83p9p37EUui4YRbD3A5-c3BkNg46
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from pandas import ExcelWriter
from pandas import ExcelFile
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
#import warnings
#warnings.filterwarnings('ignore')
sns.set(style='darkgrid')

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from warnings import filterwarnings
filterwarnings("ignore")
# %matplotlib inline

"""### **Reading in the data**"""

PCOS_inf = pd.read_csv("PCOS_infertility.csv",encoding='iso-8859-1')
PCOS_data = pd.read_excel(r'PCOS_data_without_infertility.xlsx', sheet_name='Full_new')

PCOS_data.head()

"""### **Pre-Processing**"""

#Checked how many non null values are in this 'Unnamed: 44' column
PCOS_data[~ PCOS_data['Unnamed: 44'].isna()]

"""There are only 2 non null values in this column. Hence we will drop this column later."""

#Checked the column types in this dataframe
PCOS_data.info()

"""Other than the 'Unnamed: 44' column, there are 2 other columns with null values. We will now check these columns' null rows."""

PCOS_data[PCOS_data['Marraige Status (Yrs)'].isnull()]

PCOS_data[PCOS_data['Fast food (Y/N)'].isnull()]

#lets assign the median to the missing data
PCOS_data['Marraige Status (Yrs)'].fillna(PCOS_data['Marraige Status (Yrs)'].median(),inplace=True)

PCOS_data['Fast food (Y/N)'].fillna(PCOS_data['Fast food (Y/N)'].median(),inplace=True)

PCOS_data.drop('Unnamed: 44',axis=1,inplace=True) #dropped the column

PCOS_data.isnull().sum().any() #if it returns false, it means there are no null values.

PCOS_inf.info() #checked the 2nd dataframe's information

data = pd.merge(PCOS_data,PCOS_inf, on='Sl. No', suffixes={'','_y'},how='left') #merged the dataframes on the "Sl. No" column

data.head()

#renamed the columns
data.columns = ['SNo', 'Patient_File_No.', 'PCOS_(Y/N)', 'Age_(yrs)', 'Weight_(Kg)',
       'Height(m)', 'BMI', 'Blood_Group', 'Pulse_rate(bpm)',
       'RR_(breaths/min)', 'Hb(g/dl)', 'Cycle(R/I)', 'Cycle_length(days)',
       'Marriage_Status_(Yrs)', 'Pregnant(Y/N)', 'No_of_abortions', 'I_beta-HCG(mIU/mL)_yy', 'II_beta-HCG(mIU/mL)_yy',
       'FSH(mIU/mL)', 'LH(mIU/mL)', 'FSH/LH', 'Hip(inch)', 'Waist(inch)',
       'Waist:Hip_Ratio', 'TSH_(mIU/L)', 'AMH(ng/mL)', 'PRL(ng/mL)',
       'Vit_D3_(ng/mL)', 'PRG(ng/mL)', 'RBS(mg/dl)', 'Weight_gain(Y/N)',
       'hair_growth(Y/N)', 'Skin_darkening (Y/N)', 'Hair_loss(Y/N)',
       'Pimples(Y/N)', 'Fast_food_(Y/N)', 'Reg_Exercise(Y/N)',
       'BP_Systolic(mmHg)', 'BP_Diastolic(mmHg)', 'Follicle_No.(L)',
       'Follicle_No.(R)', 'Avg.Fsize(L)(mm)', 'Avg.Fsize(R)(mm)',
       'Endometrium(mm)','Patient_File_No._y', 'PCOS(Y/N)_y',
       'I_beta-HCG(mIU/mL)', 'II_beta-HCG(mIU/mL)', 'AMH(ng/mL)_y']

#dropped one of each of the duplicate columns and two unimportant columns
data.drop(['Patient_File_No._y', 'PCOS(Y/N)_y','AMH(ng/mL)_y','SNo','Patient_File_No.','I_beta-HCG(mIU/mL)_yy', 'II_beta-HCG(mIU/mL)_yy'],axis=1,inplace=True)

"""## **Data Cleaning**"""

data.describe()  #Checking the description of columns to see if this data have outliers or irregular values

"""For each column, we are going to check outlier values on both sides according to the reseach we have done on these features."""

data.Blood_Group.value_counts() #Checked the value counts of blood group

"""For our modelling purposes, we need to convert this categorical column values into numerical type. We will create a mapping dictionary according to the column instructions provided in the excel file."""

# create a mapping dictionary for the categorical columns about blood group
blood_group_dict = {11: 'A+', 12: 'A-', 13: 'B+', 14: 'B-', 15: 'O+', 16: 'O-', 17: 'AB+', 18: 'AB-'}

data['Blood_Group'] = data['Blood_Group'].map(blood_group_dict)  # trasnform the columns into categorical
print(data.Blood_Group.value_counts(dropna = False))

data['Height(m)'] = data['Height(m)']/100  #Since the column name is in metre and actual values are in centimetres, we need to divide it by 100

data[data['Pulse_rate(bpm)'] <60]  #search for less than 60

data['Pulse_rate(bpm)'] = data['Pulse_rate(bpm)'].apply(lambda x: 70 if x<70 else x) #assign 70 to the values which is less than 70

data['Cycle(R/I)'].value_counts() #Checked the value counts

"""This column describes regular and irregular values for cycles according to the instruction. Therefore we expected this column to have binary values but we have 2,4 and only one value for 5. Our initial hypothesis is 2 is regular and 4 is irregular. Using the length of cycle days, let's see if our hypothesis is true or not."""

data.groupby('Cycle(R/I)')['Cycle_length(days)'].value_counts()

"""Our hypothesis is true as 2 has mid range values for length of days and 4 has values on extreme ends."""

data['Irregular_Cycle(Y/N)'] = data['Cycle(R/I)'].apply(lambda x: 0 if x<4 else 1) # Create a new boolean column which indicates 0=regular, 1=irregular

data.drop(columns='Cycle(R/I)',inplace=True) # Hence, dropped the old column

data[data['FSH(mIU/mL)'] >15]  #Checking outlier

data['FSH(mIU/mL)'] = data['FSH(mIU/mL)'].apply(lambda x: 4.85 if x>22 else x)  #assign the median values for extremely high values.

data[(data['LH(mIU/mL)'] >14)]  #Checked outlier

data['LH(mIU/mL)'] = data['LH(mIU/mL)'].apply(lambda x: 2.3 if x>15 else x)   #assign the median values for extremely high values.

data['FSH/LH'] = data['FSH(mIU/mL)']/ data['LH(mIU/mL)']  #Overwrite the column using the cleaned values of the two columns

data[data['TSH_(mIU/L)'] > 10]  #Checked outlier

#assigned the 75th percentile value since there are more than a dozen irregular high values
data['TSH_(mIU/L)'] = data['TSH_(mIU/L)'].apply(lambda x: 3.57 if x>10 else x)

data[data['Vit_D3_(ng/mL)'] > 90]   #Checked outlier

#we suspected these are input errors for which we divided these by 100
data['Vit_D3_(ng/mL)'] = data['Vit_D3_(ng/mL)'].apply(lambda x: x/100 if x>100 else x)

data[data['BP_Systolic(mmHg)'] < 60]   #Checked outlier

#we suspected these are input errors for which we multiplied these by 10
data['BP_Systolic(mmHg)'] = data['BP_Systolic(mmHg)'].apply(lambda x: x*10 if x<60 else x)

data[data['BP_Diastolic(mmHg)'] < 60]   #Checked outlier

#we suspected these are input errors for which we multiplied these by 10
data['BP_Diastolic(mmHg)'] = data['BP_Diastolic(mmHg)'].apply(lambda x: x*10 if x<60 else x)

data['AMH(ng/mL)'] = pd.to_numeric(data['AMH(ng/mL)'],errors='coerce')  #generated error(null) value for string in this column

data['AMH(ng/mL)'].fillna(data['AMH(ng/mL)'].median(),inplace=True)  #assigned median to the null value

df=data.rename(columns={'PCOS_(Y/N)': 'Class'})  #Renamed the target column

df.head()

#shuffle_df = df.sample(frac=1).reset_index(drop=True)
#shuffle_df.head()

cleaned_df= df.copy()  #created a new cleaned version of the dataframe
cleaned_df.head()

cleaned_df.to_csv('Cleaned_PCOS_Final.csv',index=False)  #saved the dataframe as a CSV file

